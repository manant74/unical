"""
LUMIA Studio - Setup Modelli
============================

Script per pre-scaricare i modelli necessari al funzionamento di LUMIA Studio.
Esegui questo script UNA VOLTA prima di utilizzare l'applicazione per evitare
download durante l'uso.

Uso:
    python setup_models.py

Cosa viene scaricato:
    - Modello di embedding: paraphrase-multilingual-MiniLM-L12-v2
    - Dimensioni: ~120 MB
    - Provider: HuggingFace (sentence-transformers)
"""

import sys
import os

def main():
    print("=" * 70)
    print("ğŸš€ LUMIA Studio - Inizializzazione Modelli")
    print("=" * 70)
    print()

    # Step 1: Verifica dipendenze
    print("ğŸ“¦ Step 1/3: Verifica dipendenze...")
    try:
        import sentence_transformers
        print(f"   âœ… sentence-transformers {sentence_transformers.__version__}")
    except ImportError:
        print("   âŒ sentence-transformers non installato")
        print("   ğŸ’¡ Esegui: pip install sentence-transformers")
        sys.exit(1)

    try:
        from langchain_huggingface import HuggingFaceEmbeddings
        print("   âœ… langchain-huggingface installato")
    except ImportError:
        print("   âŒ langchain-huggingface non installato")
        print("   ğŸ’¡ Esegui: pip install langchain-huggingface")
        sys.exit(1)

    print()

    # Step 2: Download modello embedding
    print("ğŸ“¥ Step 2/3: Download modello di embedding...")
    print("   Modello: paraphrase-multilingual-MiniLM-L12-v2")
    print("   Provider: HuggingFace")
    print("   Dimensioni: ~120 MB")
    print()
    print("   â³ Download in corso (potrebbe richiedere 1-2 minuti)...")
    print()

    try:
        from sentence_transformers import SentenceTransformer

        # Download del modello (mostra progress bar automaticamente)
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

        print()
        print(f"   âœ… Modello scaricato con successo!")

        # Determina la cache folder
        cache_folder = None
        if hasattr(model, 'cache_folder'):
            cache_folder = model.cache_folder
        elif hasattr(model, '_cache_folder'):
            cache_folder = model._cache_folder
        else:
            # Default HuggingFace cache location
            cache_folder = os.path.expanduser("~/.cache/huggingface/hub/")

        print(f"   ğŸ“ Cache: {cache_folder}")
        print()
    except Exception as e:
        print(f"   âŒ Errore durante il download: {str(e)}")
        sys.exit(1)

    # Step 3: Test embedding
    print("ğŸ§ª Step 3/3: Test funzionalitÃ  embedding...")
    try:
        # Test con frase italiana
        test_text = "Questo Ã¨ un test di embedding per LUMIA Studio"
        test_embedding = model.encode(test_text)

        print(f"   âœ… Test completato con successo!")
        print(f"   ğŸ“Š Dimensioni embedding: {len(test_embedding)} dimensioni")
        print(f"   ğŸ”¢ Tipo: {type(test_embedding)}")
        print()
    except Exception as e:
        print(f"   âŒ Errore durante il test: {str(e)}")
        sys.exit(1)

    # Riepilogo finale
    print("=" * 70)
    print("âœ… Setup completato con successo!")
    print("=" * 70)
    print()
    print("ğŸ“ Riepilogo:")
    print(f"   â€¢ Modello: paraphrase-multilingual-MiniLM-L12-v2")
    print(f"   â€¢ Dimensioni embedding: 384")
    print(f"   â€¢ Cache locale: {cache_folder}")
    print()
    print("ğŸ¯ Prossimi passi:")
    print("   1. Avvia LUMIA Studio: streamlit run app.py")
    print("   2. Il modello sarÃ  caricato istantaneamente dalla cache locale")
    print()
    print("ğŸ’¡ Note:")
    print("   â€¢ Il download Ã¨ necessario solo la prima volta")
    print("   â€¢ Le sessioni successive useranno la cache locale")
    print("   â€¢ Non Ã¨ necessario rieseguire questo script")
    print()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Setup interrotto dall'utente")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Errore imprevisto: {str(e)}")
        sys.exit(1)
